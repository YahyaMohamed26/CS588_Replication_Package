{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppHbg7Sz3bS4"
   },
   "source": [
    "Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QQgbpBb3VJf"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrGpkydE3gCN"
   },
   "source": [
    "Take Datasets\n",
    "\n",
    "positive_samples.csv with\n",
    "Project_id,BIC_Hash,BIC_Message,BIC_Author,BR_Title,BR_Description,BFX_Hash,BFX_Message,BFX_Author\n",
    "\n",
    "\n",
    "and Create\n",
    "\n",
    "bug_reports.csv  that contains columns:\n",
    "bug_report, related_commit_message, commit_hash\n",
    "\n",
    "all_commits.csv that contains columns:\n",
    "commit_message, commit_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeCvNPy45xsj"
   },
   "source": [
    "Generation of Negative Pairs and Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T16:42:55.608542Z",
     "start_time": "2023-12-16T16:42:53.411833Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d_/0d3mqct10593x5hgxdjxjhmh0000gn/T/ipykernel_73979/205114722.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bug_reports_train['bug_report'] = bug_reports_train['BR_Title'] + ' ' + bug_reports_train['BR_Description']\n",
      "/var/folders/d_/0d3mqct10593x5hgxdjxjhmh0000gn/T/ipykernel_73979/205114722.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bug_reports_test['bug_report'] = bug_reports_test['BR_Title'] + ' ' + bug_reports_test['BR_Description']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ratio (bug reports): 0.80\n",
      "Testing data ratio (bug reports): 0.20\n",
      "Training data ratio (all commits): 0.96\n",
      "Testing data ratio (all commits): 0.04\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load positive_samples.csv\n",
    "positive_samples = pd.read_csv('positive_samples.csv')\n",
    "\n",
    "# Splitting data for each project\n",
    "unique_projects = positive_samples['Project_id'].unique()\n",
    "train_frames = []\n",
    "test_frames = []\n",
    "all_commits = pd.DataFrame()\n",
    "\n",
    "for filename in glob.glob('Project All Commits CSVs/*.csv'):\n",
    "    project_df = pd.read_csv(filename, usecols=['Commit Hash', 'Commit Message'])\n",
    "    all_commits = pd.concat([all_commits, project_df])\n",
    "\n",
    "for project_id in unique_projects:\n",
    "    project_data = positive_samples[positive_samples['Project_id'] == project_id]\n",
    "    train, test = train_test_split(project_data, test_size=0.2, random_state=42)\n",
    "    train_frames.append(train)\n",
    "    test_frames.append(test)\n",
    "\n",
    "# Combine split data into training and testing datasets\n",
    "train_data = pd.concat(train_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "\n",
    "# Create bug_reports_train.csv and bug_reports_test.csv\n",
    "bug_reports_train = train_data[['BR_Title', 'BR_Description', 'BIC_Message', 'BIC_Hash']]\n",
    "bug_reports_train['bug_report'] = bug_reports_train['BR_Title'] + ' ' + bug_reports_train['BR_Description']\n",
    "bug_reports_train = bug_reports_train[['bug_report', 'BIC_Message', 'BIC_Hash']]\n",
    "bug_reports_train.columns = ['bug_report', 'related_commit_message', 'commit_hash']\n",
    "bug_reports_train.to_csv('bug_reports_train.csv', index=False)\n",
    "\n",
    "bug_reports_test = test_data[['BR_Title', 'BR_Description', 'BIC_Message', 'BIC_Hash', 'BFX_Author']]\n",
    "bug_reports_test['bug_report'] = bug_reports_test['BR_Title'] + ' ' + bug_reports_test['BR_Description']\n",
    "bug_reports_test = bug_reports_test[['bug_report', 'BIC_Message', 'BIC_Hash', 'BFX_Author']]\n",
    "bug_reports_test.columns = ['bug_report', 'related_commit_message', 'bug_inducing_commit_hash', 'bug_fixing_developer']\n",
    "bug_reports_test.to_csv('bug_reports_test.csv', index=False)\n",
    "\n",
    "# Filter all commits for training and testing\n",
    "all_commits = all_commits.drop_duplicates(subset=['Commit Hash'])\n",
    "all_commits.columns = ['commit_hash', 'commit_message']\n",
    "training_commit_hashes = set(train_data['BIC_Hash'])\n",
    "testing_commit_hashes = set(test_data['BIC_Hash'])\n",
    "\n",
    "# Ensure no overlap in commit hashes between training and testing\n",
    "all_commits_train = all_commits[~all_commits['commit_hash'].isin(testing_commit_hashes)]\n",
    "all_commits_test = all_commits[all_commits['commit_hash'].isin(testing_commit_hashes)]\n",
    "\n",
    "# Save all_commits_train.csv and all_commits_test.csv\n",
    "all_commits_train.to_csv('all_commits_train.csv', index=False)\n",
    "all_commits_test.to_csv('all_commits_test.csv', index=False)\n",
    "\n",
    "# Reporting the ratio\n",
    "print(f\"Training data ratio (bug reports): {len(bug_reports_train) / len(positive_samples):.2f}\")\n",
    "print(f\"Testing data ratio (bug reports): {len(bug_reports_test) / len(positive_samples):.2f}\")\n",
    "print(f\"Training data ratio (all commits): {len(all_commits_train) / len(all_commits):.2f}\")\n",
    "print(f\"Testing data ratio (all commits): {len(all_commits_test) / len(all_commits):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T16:50:42.030847Z",
     "start_time": "2023-12-16T16:43:08.792396Z"
    },
    "id": "VfoPm3N85yAR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your datasets\n",
    "bug_reports_df = pd.read_csv('bug_reports_train.csv')  # Contains columns: bug_report, related_commit_message, commit_hash\n",
    "all_commits_df = pd.read_csv('all_commits_train.csv')  # Contains columns: commit_message, commit_hash\n",
    "\n",
    "# Creating Positive Pairs\n",
    "positive_pairs = bug_reports_df[['bug_report', 'related_commit_message', 'commit_hash']]\n",
    "positive_pairs['is_related'] = 1\n",
    "\n",
    "# Creating Negative Pairs\n",
    "negative_pairs_list = []\n",
    "\n",
    "for _, row in bug_reports_df.iterrows():\n",
    "    bug_report = row['bug_report']\n",
    "    related_commit_hash = row['commit_hash']\n",
    "\n",
    "    # Randomly select commit messages which are not related\n",
    "    unrelated_commits = all_commits_df[all_commits_df['commit_hash'] != related_commit_hash].sample(n=1)\n",
    "\n",
    "    for _, unrelated_row in unrelated_commits.iterrows():\n",
    "        negative_pairs_list.append([bug_report, unrelated_row['commit_message'], unrelated_row['commit_hash'], 0])\n",
    "\n",
    "negative_pairs = pd.DataFrame(negative_pairs_list, columns=['bug_report', 'commit_message', 'commit_hash', 'is_related'])\n",
    "\n",
    "# Combining Positive and Negative Pairs\n",
    "combined_df = pd.concat([positive_pairs, negative_pairs]).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv('combined_pairs_with_hashes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AEJWSnx3FL_"
   },
   "source": [
    "Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T16:53:56.953339Z",
     "start_time": "2023-12-16T16:53:56.657673Z"
    },
    "id": "Tz0wGmFW3Ent"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yahyaelnouby/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yahyaelnouby/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special chars and numbers\n",
    "    text = re.sub(r'\\W+|\\d+', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and stemming\n",
    "    filtered_words = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkVwNu9M3XNS"
   },
   "source": [
    "Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T20:43:16.671221Z",
     "start_time": "2023-12-16T19:54:34.206643Z"
    },
    "id": "sjqYRfY137RC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3189/3189 [==============================] - 277s 87ms/step - loss: 0.0020\n",
      "Epoch 2/10\n",
      "3189/3189 [==============================] - 276s 87ms/step - loss: 1.2394e-05\n",
      "Epoch 3/10\n",
      "3189/3189 [==============================] - 274s 86ms/step - loss: 1.0589e-05\n",
      "Epoch 4/10\n",
      "3189/3189 [==============================] - 276s 87ms/step - loss: 8.7107e-06\n",
      "Epoch 5/10\n",
      "3189/3189 [==============================] - 277s 87ms/step - loss: 6.1689e-06\n",
      "Epoch 6/10\n",
      "3189/3189 [==============================] - 282s 88ms/step - loss: 4.1269e-06\n",
      "Epoch 7/10\n",
      "3189/3189 [==============================] - 287s 90ms/step - loss: 5.0695e-06\n",
      "Epoch 8/10\n",
      "3189/3189 [==============================] - 285s 89ms/step - loss: 4.4411e-06\n",
      "Epoch 9/10\n",
      "3189/3189 [==============================] - 284s 89ms/step - loss: 2.3353e-06\n",
      "Epoch 10/10\n",
      "3189/3189 [==============================] - 278s 87ms/step - loss: 2.3846e-06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, Model, optimizers\n",
    "\n",
    "# Read your combined dataset with commit hashes\n",
    "combined_df = pd.read_csv('combined_pairs_with_hashes.csv')\n",
    "\n",
    "\n",
    "combined_df['bug_report'] = combined_df['bug_report'].apply(preprocess_text)\n",
    "combined_df['commit_message'] = combined_df['commit_message'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([combined_df['bug_report'], combined_df['commit_message']]))\n",
    "\n",
    "max_len = 295  # Adjust based on your data\n",
    "seq_bug_report = pad_sequences(tokenizer.texts_to_sequences(combined_df['bug_report']), maxlen=max_len)\n",
    "seq_commit_message = pad_sequences(tokenizer.texts_to_sequences(combined_df['commit_message']), maxlen=max_len)\n",
    "\n",
    "labels = combined_df['is_related'].values\n",
    "\n",
    "# Siamese Network Architecture\n",
    "def create_model():\n",
    "    input = layers.Input(shape=(max_len,))\n",
    "    x = layers.Embedding(len(tokenizer.word_index) + 1, 128)(input)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "bug_report_model = create_model()\n",
    "commit_message_model = create_model()\n",
    "\n",
    "input_bug_report = layers.Input(shape=(max_len,))\n",
    "input_commit_message = layers.Input(shape=(max_len,))\n",
    "\n",
    "encoded_bug_report = bug_report_model(input_bug_report)\n",
    "encoded_commit_message = commit_message_model(input_commit_message)\n",
    "\n",
    "distance = layers.Lambda(lambda x: tf.norm(x[0] - x[1], axis=1))([encoded_bug_report, encoded_commit_message])\n",
    "model = Model([input_bug_report, input_commit_message], distance)\n",
    "\n",
    "# Contrastive Loss Function\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizers.Adam(), loss=contrastive_loss)\n",
    "\n",
    "# Train model\n",
    "model.fit([seq_bug_report, seq_commit_message], labels, epochs=10, batch_size=32)\n",
    "\n",
    "def find_related_commit(all_commits_csv, bug_report_text, k=5):\n",
    "    # Load all commits\n",
    "    all_commits_df = pd.read_csv(all_commits_csv)\n",
    "\n",
    "    # Preprocess the bug report text\n",
    "    processed_bug_report = preprocess_text(bug_report_text)\n",
    "    bug_seq = pad_sequences(tokenizer.texts_to_sequences([processed_bug_report]), maxlen=max_len)\n",
    "\n",
    "    # Preprocess and pad all commit messages\n",
    "    all_commits_df['commit_message'] = all_commits_df['commit_message'].apply(preprocess_text)\n",
    "    all_commit_seqs = pad_sequences(tokenizer.texts_to_sequences(all_commits_df['commit_message']), maxlen=max_len)\n",
    "\n",
    "    # Calculate similarities\n",
    "    similarities = model.predict([np.tile(bug_seq, (len(all_commit_seqs), 1)), all_commit_seqs])\n",
    "\n",
    "    # Find top 'k' similar commit hashes and messages\n",
    "    top_k_indices = np.argsort(similarities, axis=0)[:k].flatten()\n",
    "    top_k_hashes = all_commits_df.iloc[top_k_indices]['commit_hash'].values\n",
    "    top_k_messages = all_commits_df.iloc[top_k_indices]['commit_message'].values\n",
    "\n",
    "    return list(zip(top_k_hashes, top_k_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T20:47:10.693158Z",
     "start_time": "2023-12-16T20:47:10.645013Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"siamese.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T20:53:02.731879Z",
     "start_time": "2023-12-16T20:52:50.652604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"bug_reports_test.csv\", 'r') as file:\n",
    "    # Create a CSV reader\n",
    "    csv_reader = csv.DictReader(file)\n",
    "\n",
    "    \n",
    "    # Iterate through each row in the CSV file\n",
    "    k = 0\n",
    "    recall_at_10 = 0\n",
    "    for row in csv_reader:\n",
    "      bug_report = row[\"bug_report\"]\n",
    "      top_k_commits = find_related_commit('all_commits_test.csv', bug_report, k=10)\n",
    "      print(\"doğru: \", row[\"related_commit_message\"])\n",
    "      for commit_hash, commit_message in top_k_commits:\n",
    "            i=0\n",
    "            print(\"found: \", commit_message)\n",
    "            if row[\"bug_inducing_commit_hash\"] == commit_hash:\n",
    "                print(\"found bic\")\n",
    "                i=1\n",
    "            recall_at_10+=i\n",
    "    k+=1\n",
    "            \n",
    "    recall_at_10 = recall_at_10/k\n",
    "    print(recall_at_10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
